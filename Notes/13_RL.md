# Reinforcement Learning

- There are three main types of RL
    - Value Based
        - We have a state and environment
        - We are in a state, then we take an action, then we get a reward for that action
        - This can be stochastic and is a Markov Decision Process
            - Markovian means that we don't look into the past of have history, we make a decision based on our current state only
    - Policy Based
        - We have a policy that takes in state and returns an action
    - Model Based

## Value Based

- The OG vanilla version
    - We build a table that tells us the quality of an action given a state
    - The rows are the states we have, the columns are the actions we can take
    - The whole goal is to build this table
        - Once it is built, we have to just find the best action for the state we are in, not hard
    - This is not a deep learning method, but we can make it one
    - How do we build a table though?
        - We calculate the expected return of taking an action from the state we are in
            - This expected return is a discounted sum of future rewards
                - We discount the reward the further into the future we go
    - The Q-values are the sum of the current reward and the discount of the Q value of the best step out of that state
    - The amount by which we consider the future is very important
- How it works:
    - We choose an action based on our state
    - We make the action and get the reward
    - We update our table based on the reward and such
    - We update our state
    - We choose an action
    - ...


## Policy Based

- This is different in that we are no longer learning a hard-coded thing with our model, a hard-coded decision.
- Instead, we are learning a mapping of probability distribution in order to get a good, stochastic, policy
- There are two different ways to do this training:
    - On-policy: the model is the agent, it acts and learns that way, it doesn't keep data from past training and so has to spend a lot of time exploring
    - Off-policy: the model gets to learn from a database of actions and such that have been provided to it without getting to explore itself
- The OG vanilla version:
    - We have two networks: us and our buddy
    - we have a policy that we take some number of time steps in from a number of different trajectories
    - We get the reward for each time step n each trajectory
    - We get the buddy's prediction of the reward we should have gotten in our time step and compare that to what we actually got
    - We try to get our buddy to do his best to make that discrepancy (called the advantage) as small as possible
    - We, then, compute our probability and scale it by the advantage for the time step, and gradient ascend on it
- We want to make sure that we don't overfit on one batch because this could take us to a point of no return where our network is no longer good
    - There is a way to correct for this that involves KL divergence, but we have a quicker, hacky way to do this with clipping
    - This is what we call PPO
- Here is how we do PPO:
    - 
